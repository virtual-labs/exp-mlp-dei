<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">@import url(https://themes.googleusercontent.com/fonts/css?kit=fpjTOVmNbO4Lz34iLyptLWOEp5l7D1DtylGw-U95tLLctg_Dj5nIQZ5ehwU-wE9eo1EtIZ9LpswtF5iguWh9wg);.lst-kix_list_14-1>li:before{content:"" counter(lst-ctn-kix_list_14-1,lower-latin) ". "}.lst-kix_list_14-3>li:before{content:"" counter(lst-ctn-kix_list_14-3,decimal) ". "}.lst-kix_list_2-1>li{counter-increment:lst-ctn-kix_list_2-1}.lst-kix_list_14-0>li:before{content:"" counter(lst-ctn-kix_list_14-0,decimal) ". "}.lst-kix_list_14-4>li:before{content:"" counter(lst-ctn-kix_list_14-4,lower-latin) ". "}.lst-kix_list_14-5>li:before{content:"" counter(lst-ctn-kix_list_14-5,lower-roman) ". "}.lst-kix_list_14-7>li:before{content:"" counter(lst-ctn-kix_list_14-7,lower-latin) ". "}ol.lst-kix_list_13-4.start{counter-reset:lst-ctn-kix_list_13-4 0}.lst-kix_list_14-6>li:before{content:"" counter(lst-ctn-kix_list_14-6,decimal) ". "}ul.lst-kix_list_9-3{list-style-type:none}ul.lst-kix_list_9-4{list-style-type:none}ul.lst-kix_list_9-1{list-style-type:none}ul.lst-kix_list_9-2{list-style-type:none}ul.lst-kix_list_9-7{list-style-type:none}.lst-kix_list_13-0>li{counter-increment:lst-ctn-kix_list_13-0}ul.lst-kix_list_9-8{list-style-type:none}ul.lst-kix_list_9-5{list-style-type:none}ul.lst-kix_list_9-6{list-style-type:none}ol.lst-kix_list_2-3.start{counter-reset:lst-ctn-kix_list_2-3 0}ul.lst-kix_list_9-0{list-style-type:none}.lst-kix_list_14-2>li:before{content:"" counter(lst-ctn-kix_list_14-2,lower-roman) ". "}ul.lst-kix_list_17-1{list-style-type:none}ul.lst-kix_list_17-0{list-style-type:none}ul.lst-kix_list_17-8{list-style-type:none}ul.lst-kix_list_17-7{list-style-type:none}ul.lst-kix_list_17-6{list-style-type:none}ul.lst-kix_list_17-5{list-style-type:none}ul.lst-kix_list_17-4{list-style-type:none}ul.lst-kix_list_17-3{list-style-type:none}.lst-kix_list_14-8>li:before{content:"" counter(lst-ctn-kix_list_14-8,lower-roman) ". "}ul.lst-kix_list_17-2{list-style-type:none}.lst-kix_list_5-0>li:before{content:"\0025cf   "}.lst-kix_list_14-8>li{counter-increment:lst-ctn-kix_list_14-8}.lst-kix_list_5-3>li:before{content:"\0025cf   "}.lst-kix_list_5-2>li:before{content:"\0025aa   "}.lst-kix_list_5-1>li:before{content:"o  "}.lst-kix_list_5-7>li:before{content:"o  "}ul.lst-kix_list_8-4{list-style-type:none}ul.lst-kix_list_8-5{list-style-type:none}.lst-kix_list_5-6>li:before{content:"\0025cf   "}.lst-kix_list_5-8>li:before{content:"\0025aa   "}ul.lst-kix_list_8-2{list-style-type:none}ul.lst-kix_list_8-3{list-style-type:none}ul.lst-kix_list_8-8{list-style-type:none}ul.lst-kix_list_8-6{list-style-type:none}ul.lst-kix_list_8-7{list-style-type:none}.lst-kix_list_5-4>li:before{content:"o  "}.lst-kix_list_5-5>li:before{content:"\0025aa   "}ul.lst-kix_list_8-0{list-style-type:none}ul.lst-kix_list_8-1{list-style-type:none}ol.lst-kix_list_14-1.start{counter-reset:lst-ctn-kix_list_14-1 0}.lst-kix_list_6-1>li:before{content:"o  "}.lst-kix_list_6-3>li:before{content:"\0025cf   "}.lst-kix_list_6-0>li:before{content:"\0025cf   "}.lst-kix_list_6-4>li:before{content:"o  "}ol.lst-kix_list_14-8.start{counter-reset:lst-ctn-kix_list_14-8 0}ul.lst-kix_list_16-2{list-style-type:none}ul.lst-kix_list_16-1{list-style-type:none}ul.lst-kix_list_16-0{list-style-type:none}.lst-kix_list_6-2>li:before{content:"\0025aa   "}ul.lst-kix_list_16-8{list-style-type:none}ul.lst-kix_list_16-7{list-style-type:none}ul.lst-kix_list_16-6{list-style-type:none}.lst-kix_list_2-5>li{counter-increment:lst-ctn-kix_list_2-5}.lst-kix_list_2-8>li{counter-increment:lst-ctn-kix_list_2-8}ul.lst-kix_list_16-5{list-style-type:none}ul.lst-kix_list_16-4{list-style-type:none}.lst-kix_list_6-8>li:before{content:"\0025aa   "}ul.lst-kix_list_16-3{list-style-type:none}.lst-kix_list_6-5>li:before{content:"\0025aa   "}.lst-kix_list_6-7>li:before{content:"o  "}.lst-kix_list_6-6>li:before{content:"\0025cf   "}.lst-kix_list_7-4>li:before{content:"o  "}.lst-kix_list_7-6>li:before{content:"\0025cf   "}ol.lst-kix_list_19-7.start{counter-reset:lst-ctn-kix_list_19-7 0}.lst-kix_list_7-2>li:before{content:"\0025aa   "}.lst-kix_list_13-7>li:before{content:"" counter(lst-ctn-kix_list_13-7,lower-latin) ". "}.lst-kix_list_7-8>li:before{content:"\0025aa   "}ol.lst-kix_list_2-5.start{counter-reset:lst-ctn-kix_list_2-5 0}.lst-kix_list_15-5>li:before{content:"\0025aa   "}.lst-kix_list_13-4>li{counter-increment:lst-ctn-kix_list_13-4}.lst-kix_list_4-1>li:before{content:"o  "}.lst-kix_list_15-7>li:before{content:"o  "}.lst-kix_list_4-3>li:before{content:"\0025cf   "}.lst-kix_list_4-5>li:before{content:"\0025aa   "}.lst-kix_list_15-1>li:before{content:"o  "}.lst-kix_list_15-3>li:before{content:"\0025cf   "}.lst-kix_list_19-2>li{counter-increment:lst-ctn-kix_list_19-2}ol.lst-kix_list_2-8.start{counter-reset:lst-ctn-kix_list_2-8 0}.lst-kix_list_12-3>li:before{content:"\0025cf   "}.lst-kix_list_12-1>li:before{content:"o  "}.lst-kix_list_13-3>li{counter-increment:lst-ctn-kix_list_13-3}ol.lst-kix_list_13-6.start{counter-reset:lst-ctn-kix_list_13-6 0}.lst-kix_list_14-1>li{counter-increment:lst-ctn-kix_list_14-1}.lst-kix_list_13-3>li:before{content:"" counter(lst-ctn-kix_list_13-3,decimal) ". "}ul.lst-kix_list_18-0{list-style-type:none}.lst-kix_list_13-5>li:before{content:"" counter(lst-ctn-kix_list_13-5,lower-roman) ". "}.lst-kix_list_12-5>li:before{content:"\0025aa   "}ul.lst-kix_list_18-8{list-style-type:none}ol.lst-kix_list_13-7.start{counter-reset:lst-ctn-kix_list_13-7 0}ul.lst-kix_list_18-7{list-style-type:none}ul.lst-kix_list_18-6{list-style-type:none}ul.lst-kix_list_18-5{list-style-type:none}.lst-kix_list_12-7>li:before{content:"o  "}ul.lst-kix_list_18-4{list-style-type:none}ul.lst-kix_list_18-3{list-style-type:none}ul.lst-kix_list_18-2{list-style-type:none}ul.lst-kix_list_18-1{list-style-type:none}.lst-kix_list_13-1>li:before{content:"" counter(lst-ctn-kix_list_13-1,lower-latin) ". "}ol.lst-kix_list_19-0.start{counter-reset:lst-ctn-kix_list_19-0 0}ol.lst-kix_list_2-6.start{counter-reset:lst-ctn-kix_list_2-6 0}.lst-kix_list_3-0>li:before{content:"\0025cf   "}ul.lst-kix_list_5-7{list-style-type:none}ol.lst-kix_list_13-1.start{counter-reset:lst-ctn-kix_list_13-1 0}ul.lst-kix_list_5-8{list-style-type:none}ul.lst-kix_list_5-5{list-style-type:none}ul.lst-kix_list_5-6{list-style-type:none}ul.lst-kix_list_5-0{list-style-type:none}.lst-kix_list_3-4>li:before{content:"o  "}ul.lst-kix_list_5-3{list-style-type:none}.lst-kix_list_3-3>li:before{content:"\0025cf   "}ul.lst-kix_list_5-4{list-style-type:none}ul.lst-kix_list_5-1{list-style-type:none}.lst-kix_list_8-0>li:before{content:"\0025cf   "}ul.lst-kix_list_5-2{list-style-type:none}.lst-kix_list_8-7>li:before{content:"o  "}.lst-kix_list_3-8>li:before{content:"\0025aa   "}.lst-kix_list_13-1>li{counter-increment:lst-ctn-kix_list_13-1}.lst-kix_list_8-3>li:before{content:"\0025cf   "}.lst-kix_list_3-7>li:before{content:"o  "}.lst-kix_list_8-4>li:before{content:"o  "}.lst-kix_list_19-1>li{counter-increment:lst-ctn-kix_list_19-1}.lst-kix_list_11-1>li:before{content:"o  "}.lst-kix_list_11-0>li:before{content:"\0025cf   "}.lst-kix_list_8-8>li:before{content:"\0025aa   "}ol.lst-kix_list_2-2{list-style-type:none}.lst-kix_list_16-8>li:before{content:"\0025aa   "}ol.lst-kix_list_2-3{list-style-type:none}ol.lst-kix_list_2-4{list-style-type:none}.lst-kix_list_16-7>li:before{content:"o  "}ol.lst-kix_list_2-5{list-style-type:none}ol.lst-kix_list_2-0{list-style-type:none}ol.lst-kix_list_2-1{list-style-type:none}ol.lst-kix_list_19-5.start{counter-reset:lst-ctn-kix_list_19-5 0}.lst-kix_list_4-8>li:before{content:"\0025aa   "}.lst-kix_list_4-7>li:before{content:"o  "}.lst-kix_list_14-2>li{counter-increment:lst-ctn-kix_list_14-2}.lst-kix_list_17-0>li:before{content:"\0025cf   "}ul.lst-kix_list_4-8{list-style-type:none}.lst-kix_list_16-0>li:before{content:"\0025cf   "}ul.lst-kix_list_4-6{list-style-type:none}ul.lst-kix_list_4-7{list-style-type:none}ul.lst-kix_list_4-0{list-style-type:none}.lst-kix_list_16-4>li:before{content:"o  "}ul.lst-kix_list_4-1{list-style-type:none}.lst-kix_list_16-3>li:before{content:"\0025cf   "}ul.lst-kix_list_4-4{list-style-type:none}ol.lst-kix_list_2-6{list-style-type:none}ul.lst-kix_list_4-5{list-style-type:none}ol.lst-kix_list_2-7{list-style-type:none}ul.lst-kix_list_4-2{list-style-type:none}ol.lst-kix_list_2-8{list-style-type:none}ul.lst-kix_list_4-3{list-style-type:none}ul.lst-kix_list_12-6{list-style-type:none}ul.lst-kix_list_12-5{list-style-type:none}.lst-kix_list_17-7>li:before{content:"o  "}ul.lst-kix_list_12-4{list-style-type:none}ul.lst-kix_list_12-3{list-style-type:none}ul.lst-kix_list_12-2{list-style-type:none}ul.lst-kix_list_12-1{list-style-type:none}.lst-kix_list_17-8>li:before{content:"\0025aa   "}ul.lst-kix_list_12-0{list-style-type:none}.lst-kix_list_17-3>li:before{content:"\0025cf   "}.lst-kix_list_17-4>li:before{content:"o  "}ul.lst-kix_list_12-8{list-style-type:none}ul.lst-kix_list_12-7{list-style-type:none}.lst-kix_list_7-0>li:before{content:"\0025cf   "}.lst-kix_list_13-8>li{counter-increment:lst-ctn-kix_list_13-8}.lst-kix_list_2-2>li{counter-increment:lst-ctn-kix_list_2-2}ol.lst-kix_list_19-6.start{counter-reset:lst-ctn-kix_list_19-6 0}ol.lst-kix_list_13-8{list-style-type:none}.lst-kix_list_2-4>li:before{content:"" counter(lst-ctn-kix_list_2-4,lower-latin) ". "}.lst-kix_list_2-8>li:before{content:"" counter(lst-ctn-kix_list_2-8,lower-roman) ". "}ol.lst-kix_list_13-4{list-style-type:none}ol.lst-kix_list_13-5{list-style-type:none}ol.lst-kix_list_13-6{list-style-type:none}ol.lst-kix_list_13-7{list-style-type:none}ol.lst-kix_list_13-0{list-style-type:none}ol.lst-kix_list_13-1{list-style-type:none}ol.lst-kix_list_13-2{list-style-type:none}.lst-kix_list_7-3>li:before{content:"\0025cf   "}ol.lst-kix_list_13-3{list-style-type:none}ul.lst-kix_list_7-5{list-style-type:none}.lst-kix_list_10-0>li:before{content:"\0025cf   "}ul.lst-kix_list_7-6{list-style-type:none}ul.lst-kix_list_7-3{list-style-type:none}ul.lst-kix_list_7-4{list-style-type:none}.lst-kix_list_13-6>li{counter-increment:lst-ctn-kix_list_13-6}.lst-kix_list_13-8>li:before{content:"" counter(lst-ctn-kix_list_13-8,lower-roman) ". "}ol.lst-kix_list_14-6.start{counter-reset:lst-ctn-kix_list_14-6 0}.lst-kix_list_18-3>li:before{content:"\0025cf   "}.lst-kix_list_18-7>li:before{content:"o  "}ul.lst-kix_list_7-7{list-style-type:none}ul.lst-kix_list_7-8{list-style-type:none}ul.lst-kix_list_7-1{list-style-type:none}.lst-kix_list_19-6>li{counter-increment:lst-ctn-kix_list_19-6}ul.lst-kix_list_7-2{list-style-type:none}ul.lst-kix_list_7-0{list-style-type:none}.lst-kix_list_7-7>li:before{content:"o  "}.lst-kix_list_15-4>li:before{content:"o  "}ol.lst-kix_list_19-1.start{counter-reset:lst-ctn-kix_list_19-1 0}.lst-kix_list_10-4>li:before{content:"o  "}.lst-kix_list_10-8>li:before{content:"\0025aa   "}.lst-kix_list_4-0>li:before{content:"\0025cf   "}ul.lst-kix_list_15-3{list-style-type:none}ul.lst-kix_list_15-2{list-style-type:none}.lst-kix_list_15-0>li:before{content:"\0025cf   "}ul.lst-kix_list_15-1{list-style-type:none}.lst-kix_list_15-8>li:before{content:"\0025aa   "}ul.lst-kix_list_15-0{list-style-type:none}ol.lst-kix_list_19-4.start{counter-reset:lst-ctn-kix_list_19-4 0}ol.lst-kix_list_14-3.start{counter-reset:lst-ctn-kix_list_14-3 0}.lst-kix_list_4-4>li:before{content:"o  "}ol.lst-kix_list_2-2.start{counter-reset:lst-ctn-kix_list_2-2 0}.lst-kix_list_19-4>li{counter-increment:lst-ctn-kix_list_19-4}ul.lst-kix_list_15-8{list-style-type:none}ul.lst-kix_list_15-7{list-style-type:none}ul.lst-kix_list_15-6{list-style-type:none}.lst-kix_list_9-3>li:before{content:"\0025cf   "}ul.lst-kix_list_15-5{list-style-type:none}ul.lst-kix_list_15-4{list-style-type:none}ol.lst-kix_list_13-2.start{counter-reset:lst-ctn-kix_list_13-2 0}ol.lst-kix_list_14-7{list-style-type:none}ol.lst-kix_list_14-4.start{counter-reset:lst-ctn-kix_list_14-4 0}ol.lst-kix_list_14-8{list-style-type:none}.lst-kix_list_9-7>li:before{content:"o  "}.lst-kix_list_2-4>li{counter-increment:lst-ctn-kix_list_2-4}ol.lst-kix_list_14-3{list-style-type:none}ol.lst-kix_list_14-4{list-style-type:none}ol.lst-kix_list_14-5{list-style-type:none}ol.lst-kix_list_14-6{list-style-type:none}.lst-kix_list_11-4>li:before{content:"o  "}ol.lst-kix_list_14-0{list-style-type:none}.lst-kix_list_12-4>li:before{content:"o  "}ol.lst-kix_list_14-1{list-style-type:none}ol.lst-kix_list_14-2{list-style-type:none}ul.lst-kix_list_6-6{list-style-type:none}ul.lst-kix_list_6-7{list-style-type:none}ul.lst-kix_list_6-4{list-style-type:none}ul.lst-kix_list_6-5{list-style-type:none}ul.lst-kix_list_6-8{list-style-type:none}.lst-kix_list_1-0>li:before{content:"\0025cf   "}ol.lst-kix_list_19-2.start{counter-reset:lst-ctn-kix_list_19-2 0}ul.lst-kix_list_6-2{list-style-type:none}.lst-kix_list_11-8>li:before{content:"\0025aa   "}ul.lst-kix_list_6-3{list-style-type:none}ol.lst-kix_list_2-0.start{counter-reset:lst-ctn-kix_list_2-0 0}ul.lst-kix_list_6-0{list-style-type:none}.lst-kix_list_12-0>li:before{content:"\0025cf   "}ul.lst-kix_list_6-1{list-style-type:none}.lst-kix_list_1-4>li:before{content:"o  "}.lst-kix_list_13-0>li:before{content:"" counter(lst-ctn-kix_list_13-0,decimal) ". "}ol.lst-kix_list_13-0.start{counter-reset:lst-ctn-kix_list_13-0 0}.lst-kix_list_14-4>li{counter-increment:lst-ctn-kix_list_14-4}.lst-kix_list_13-4>li:before{content:"" counter(lst-ctn-kix_list_13-4,lower-latin) ". "}ol.lst-kix_list_19-3.start{counter-reset:lst-ctn-kix_list_19-3 0}.lst-kix_list_2-0>li:before{content:"" counter(lst-ctn-kix_list_2-0,decimal) ". "}ol.lst-kix_list_2-1.start{counter-reset:lst-ctn-kix_list_2-1 0}ol.lst-kix_list_14-5.start{counter-reset:lst-ctn-kix_list_14-5 0}.lst-kix_list_1-8>li:before{content:"\0025aa   "}.lst-kix_list_12-8>li:before{content:"\0025aa   "}.lst-kix_list_19-0>li:before{content:"" counter(lst-ctn-kix_list_19-0,upper-roman) ". "}.lst-kix_list_19-1>li:before{content:"" counter(lst-ctn-kix_list_19-1,lower-latin) ". "}ul.lst-kix_list_1-0{list-style-type:none}.lst-kix_list_19-4>li:before{content:"" counter(lst-ctn-kix_list_19-4,lower-latin) ". "}.lst-kix_list_19-2>li:before{content:"" counter(lst-ctn-kix_list_19-2,lower-roman) ". "}.lst-kix_list_19-3>li:before{content:"" counter(lst-ctn-kix_list_19-3,decimal) ". "}ul.lst-kix_list_1-3{list-style-type:none}ul.lst-kix_list_1-4{list-style-type:none}.lst-kix_list_19-0>li{counter-increment:lst-ctn-kix_list_19-0}ul.lst-kix_list_1-1{list-style-type:none}ul.lst-kix_list_1-2{list-style-type:none}ul.lst-kix_list_1-7{list-style-type:none}ul.lst-kix_list_1-8{list-style-type:none}ul.lst-kix_list_1-5{list-style-type:none}ul.lst-kix_list_1-6{list-style-type:none}.lst-kix_list_2-3>li{counter-increment:lst-ctn-kix_list_2-3}ol.lst-kix_list_19-8.start{counter-reset:lst-ctn-kix_list_19-8 0}.lst-kix_list_19-8>li:before{content:"" counter(lst-ctn-kix_list_19-8,lower-roman) ". "}ol.lst-kix_list_14-7.start{counter-reset:lst-ctn-kix_list_14-7 0}.lst-kix_list_19-5>li:before{content:"" counter(lst-ctn-kix_list_19-5,lower-roman) ". "}.lst-kix_list_19-6>li:before{content:"" counter(lst-ctn-kix_list_19-6,decimal) ". "}.lst-kix_list_19-7>li:before{content:"" counter(lst-ctn-kix_list_19-7,lower-latin) ". "}.lst-kix_list_13-2>li{counter-increment:lst-ctn-kix_list_13-2}.lst-kix_list_19-7>li{counter-increment:lst-ctn-kix_list_19-7}.lst-kix_list_14-3>li{counter-increment:lst-ctn-kix_list_14-3}.lst-kix_list_18-0>li:before{content:"\0025cf   "}ol.lst-kix_list_13-3.start{counter-reset:lst-ctn-kix_list_13-3 0}.lst-kix_list_18-1>li:before{content:"o  "}.lst-kix_list_18-2>li:before{content:"\0025aa   "}ol.lst-kix_list_14-2.start{counter-reset:lst-ctn-kix_list_14-2 0}ol.lst-kix_list_2-4.start{counter-reset:lst-ctn-kix_list_2-4 0}.lst-kix_list_2-7>li:before{content:"" counter(lst-ctn-kix_list_2-7,lower-latin) ". "}.lst-kix_list_2-7>li{counter-increment:lst-ctn-kix_list_2-7}.lst-kix_list_2-5>li:before{content:"" counter(lst-ctn-kix_list_2-5,lower-roman) ". "}.lst-kix_list_18-6>li:before{content:"\0025cf   "}ul.lst-kix_list_3-7{list-style-type:none}.lst-kix_list_14-6>li{counter-increment:lst-ctn-kix_list_14-6}ul.lst-kix_list_3-8{list-style-type:none}.lst-kix_list_10-1>li:before{content:"o  "}.lst-kix_list_18-4>li:before{content:"o  "}.lst-kix_list_18-8>li:before{content:"\0025aa   "}ul.lst-kix_list_3-1{list-style-type:none}ul.lst-kix_list_3-2{list-style-type:none}ul.lst-kix_list_3-0{list-style-type:none}ul.lst-kix_list_3-5{list-style-type:none}ul.lst-kix_list_3-6{list-style-type:none}ul.lst-kix_list_3-3{list-style-type:none}ul.lst-kix_list_3-4{list-style-type:none}.lst-kix_list_10-7>li:before{content:"o  "}.lst-kix_list_10-5>li:before{content:"\0025aa   "}ol.lst-kix_list_13-5.start{counter-reset:lst-ctn-kix_list_13-5 0}.lst-kix_list_10-3>li:before{content:"\0025cf   "}ul.lst-kix_list_11-7{list-style-type:none}ul.lst-kix_list_11-6{list-style-type:none}.lst-kix_list_2-6>li{counter-increment:lst-ctn-kix_list_2-6}ul.lst-kix_list_11-5{list-style-type:none}ul.lst-kix_list_11-4{list-style-type:none}ul.lst-kix_list_11-3{list-style-type:none}ol.lst-kix_list_13-8.start{counter-reset:lst-ctn-kix_list_13-8 0}ul.lst-kix_list_11-2{list-style-type:none}ul.lst-kix_list_11-1{list-style-type:none}ul.lst-kix_list_11-0{list-style-type:none}.lst-kix_list_9-2>li:before{content:"\0025aa   "}ol.lst-kix_list_14-0.start{counter-reset:lst-ctn-kix_list_14-0 0}ul.lst-kix_list_11-8{list-style-type:none}.lst-kix_list_9-0>li:before{content:"\0025cf   "}.lst-kix_list_9-6>li:before{content:"\0025cf   "}.lst-kix_list_9-4>li:before{content:"o  "}.lst-kix_list_11-3>li:before{content:"\0025cf   "}.lst-kix_list_11-5>li:before{content:"\0025aa   "}.lst-kix_list_9-8>li:before{content:"\0025aa   "}.lst-kix_list_1-1>li:before{content:"o  "}.lst-kix_list_11-7>li:before{content:"o  "}ul.lst-kix_list_10-0{list-style-type:none}.lst-kix_list_1-3>li:before{content:"\0025cf   "}ul.lst-kix_list_10-8{list-style-type:none}ul.lst-kix_list_10-7{list-style-type:none}.lst-kix_list_1-7>li:before{content:"o  "}ul.lst-kix_list_10-6{list-style-type:none}ol.lst-kix_list_2-7.start{counter-reset:lst-ctn-kix_list_2-7 0}ul.lst-kix_list_10-5{list-style-type:none}ul.lst-kix_list_10-4{list-style-type:none}ul.lst-kix_list_10-3{list-style-type:none}.lst-kix_list_1-5>li:before{content:"\0025aa   "}ul.lst-kix_list_10-2{list-style-type:none}ul.lst-kix_list_10-1{list-style-type:none}.lst-kix_list_14-7>li{counter-increment:lst-ctn-kix_list_14-7}.lst-kix_list_2-1>li:before{content:"" counter(lst-ctn-kix_list_2-1,lower-latin) ". "}.lst-kix_list_19-8>li{counter-increment:lst-ctn-kix_list_19-8}.lst-kix_list_2-3>li:before{content:"" counter(lst-ctn-kix_list_2-3,decimal) ". "}ol.lst-kix_list_19-6{list-style-type:none}ol.lst-kix_list_19-7{list-style-type:none}ol.lst-kix_list_19-8{list-style-type:none}ol.lst-kix_list_19-2{list-style-type:none}ol.lst-kix_list_19-3{list-style-type:none}ol.lst-kix_list_19-4{list-style-type:none}ol.lst-kix_list_19-5{list-style-type:none}ol.lst-kix_list_19-0{list-style-type:none}ol.lst-kix_list_19-1{list-style-type:none}.lst-kix_list_3-1>li:before{content:"o  "}.lst-kix_list_3-2>li:before{content:"\0025aa   "}.lst-kix_list_14-0>li{counter-increment:lst-ctn-kix_list_14-0}.lst-kix_list_8-1>li:before{content:"o  "}.lst-kix_list_8-2>li:before{content:"\0025aa   "}.lst-kix_list_3-5>li:before{content:"\0025aa   "}.lst-kix_list_8-5>li:before{content:"\0025aa   "}.lst-kix_list_8-6>li:before{content:"\0025cf   "}.lst-kix_list_2-0>li{counter-increment:lst-ctn-kix_list_2-0}.lst-kix_list_3-6>li:before{content:"\0025cf   "}.lst-kix_list_11-2>li:before{content:"\0025aa   "}.lst-kix_list_16-6>li:before{content:"\0025cf   "}.lst-kix_list_17-1>li:before{content:"o  "}.lst-kix_list_16-1>li:before{content:"o  "}.lst-kix_list_16-2>li:before{content:"\0025aa   "}.lst-kix_list_16-5>li:before{content:"\0025aa   "}.lst-kix_list_19-3>li{counter-increment:lst-ctn-kix_list_19-3}.lst-kix_list_17-2>li:before{content:"\0025aa   "}.lst-kix_list_17-6>li:before{content:"\0025cf   "}.lst-kix_list_17-5>li:before{content:"\0025aa   "}.lst-kix_list_2-6>li:before{content:"" counter(lst-ctn-kix_list_2-6,decimal) ". "}.lst-kix_list_14-5>li{counter-increment:lst-ctn-kix_list_14-5}.lst-kix_list_7-1>li:before{content:"o  "}.lst-kix_list_7-5>li:before{content:"\0025aa   "}.lst-kix_list_13-5>li{counter-increment:lst-ctn-kix_list_13-5}.lst-kix_list_19-5>li{counter-increment:lst-ctn-kix_list_19-5}.lst-kix_list_18-5>li:before{content:"\0025aa   "}.lst-kix_list_13-6>li:before{content:"" counter(lst-ctn-kix_list_13-6,decimal) ". "}.lst-kix_list_15-6>li:before{content:"\0025cf   "}.lst-kix_list_10-2>li:before{content:"\0025aa   "}.lst-kix_list_13-7>li{counter-increment:lst-ctn-kix_list_13-7}.lst-kix_list_4-2>li:before{content:"\0025aa   "}.lst-kix_list_4-6>li:before{content:"\0025cf   "}.lst-kix_list_15-2>li:before{content:"\0025aa   "}.lst-kix_list_10-6>li:before{content:"\0025cf   "}.lst-kix_list_9-1>li:before{content:"o  "}.lst-kix_list_9-5>li:before{content:"\0025aa   "}.lst-kix_list_12-2>li:before{content:"\0025aa   "}.lst-kix_list_11-6>li:before{content:"\0025cf   "}.lst-kix_list_1-2>li:before{content:"\0025aa   "}.lst-kix_list_1-6>li:before{content:"\0025cf   "}li.li-bullet-0:before{margin-left:-18pt;white-space:nowrap;display:inline-block;min-width:18pt}.lst-kix_list_12-6>li:before{content:"\0025cf   "}.lst-kix_list_2-2>li:before{content:"" counter(lst-ctn-kix_list_2-2,lower-roman) ". "}.lst-kix_list_13-2>li:before{content:"" counter(lst-ctn-kix_list_13-2,lower-roman) ". "}ol{margin:0;padding:0}table td,table th{padding:0}.c24{border-right-style:solid;padding:0pt 5.8pt 0pt 5.8pt;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:156pt;border-top-color:#000000;border-bottom-style:solid}.c38{-webkit-text-decoration-skip:none;color:#000000;font-weight:700;text-decoration:underline;vertical-align:baseline;text-decoration-skip-ink:none;font-size:14pt;font-family:"Calibri";font-style:normal}.c2{margin-left:36pt;padding-top:0pt;padding-bottom:0pt;line-height:1.1624999999999999;orphans:2;widows:2;text-align:left}.c21{color:#1f1f1f;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10.5pt;font-family:"Calibri";font-style:normal}.c14{margin-left:18pt;padding-top:0pt;padding-bottom:0pt;line-height:1.0708333333333333;orphans:2;widows:2;text-align:left}.c11{padding-top:0pt;padding-bottom:0pt;line-height:1.0;orphans:2;widows:2;text-align:right;margin-right:-5.8pt}.c16{margin-left:72pt;padding-top:0pt;padding-bottom:0pt;line-height:1.0708333333333333;orphans:2;widows:2;text-align:left}.c15{color:#44546a;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:9pt;font-family:"Calibri";font-style:italic}.c3{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Aptos";font-style:normal}.c7{margin-left:36pt;padding-top:0pt;padding-bottom:0pt;line-height:1.0708333333333333;orphans:2;widows:2;text-align:left}.c27{margin-left:36pt;padding-top:12pt;padding-bottom:0pt;line-height:1.1624999999999999;orphans:2;widows:2;text-align:left}.c4{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Calibri";font-style:normal}.c32{color:#1f1f1f;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Aptos";font-style:normal}.c50{margin-left:-5.8pt;padding-top:0pt;padding-bottom:0pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c0{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Calibri";font-style:normal}.c13{color:#000000;font-weight:700;text-decoration:none;vertical-align:sub;font-size:11pt;font-family:"Calibri";font-style:normal}.c53{padding-top:14.9pt;padding-bottom:14.9pt;line-height:1.1624999999999999;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c9{padding-top:0pt;padding-bottom:0pt;line-height:1.1624999999999999;orphans:2;widows:2;text-align:left}.c49{padding-top:0pt;padding-bottom:0pt;line-height:1.0;orphans:2;widows:2;text-align:center}.c6{padding-top:11pt;padding-bottom:8pt;line-height:1.0708333333333333;orphans:2;widows:2;text-align:left}.c55{padding-top:11pt;padding-bottom:0pt;line-height:1.0708333333333333;orphans:2;widows:2;text-align:left}.c12{padding-top:0pt;padding-bottom:8pt;line-height:1.1624999999999999;orphans:2;widows:2;text-align:left}.c37{padding-top:0pt;padding-bottom:12pt;line-height:1.0708333333333333;orphans:2;widows:2;text-align:left}.c26{padding-top:9pt;padding-bottom:0pt;line-height:1.1624999999999999;orphans:2;widows:2;text-align:center}.c65{padding-top:0pt;padding-bottom:0pt;line-height:1.1624999999999999;orphans:2;widows:2;text-align:center}.c18{padding-top:12pt;padding-bottom:12pt;line-height:1.1624999999999999;orphans:2;widows:2;text-align:left}.c22{padding-top:11pt;padding-bottom:8pt;line-height:1.0708333333333333;orphans:2;widows:2;text-align:center}.c41{padding-top:0pt;padding-bottom:8pt;line-height:1.0708333333333333;orphans:2;widows:2;text-align:left}.c34{padding-top:0pt;padding-bottom:12pt;line-height:1.1624999999999999;orphans:2;widows:2;text-align:left}.c39{padding-top:12pt;padding-bottom:0pt;line-height:1.0708333333333333;orphans:2;widows:2;text-align:left}.c30{padding-top:0pt;padding-bottom:0pt;line-height:1.1875;orphans:2;widows:2;text-align:left}.c43{padding-top:0pt;padding-bottom:8pt;line-height:1.1624999999999999;orphans:2;widows:2;text-align:center}.c56{padding-top:14.9pt;padding-bottom:14.9pt;line-height:1.0708333333333333;orphans:2;widows:2;text-align:left}.c23{padding-top:0pt;padding-bottom:0pt;line-height:1.0708333333333333;orphans:2;widows:2;text-align:left}.c64{padding-top:9pt;padding-bottom:0pt;line-height:1.1624999999999999;orphans:2;widows:2;text-align:left}.c68{padding-top:0pt;padding-bottom:0pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c10{font-size:11pt;font-family:"Calibri";font-style:italic;color:#000000;font-weight:400}.c25{font-size:11pt;font-family:"Calibri";font-style:normal;color:#000000;font-weight:400}.c19{font-size:11pt;font-family:"Cambria Math";font-style:italic;font-weight:400}.c47{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c8{border-spacing:0;border-collapse:collapse;margin-right:auto}.c40{font-size:9pt;font-family:"Calibri";color:#000000;font-weight:400}.c33{color:#333333;text-decoration:none;vertical-align:baseline;font-style:normal}.c66{font-weight:700;font-size:12pt;font-family:"Calibri"}.c29{font-size:11pt;font-weight:400;font-family:"Calibri"}.c46{text-decoration-skip-ink:none;-webkit-text-decoration-skip:none;text-decoration:underline}.c36{font-size:11pt;font-weight:700;font-family:"Calibri"}.c59{font-size:9pt;font-weight:400;font-family:"Calibri"}.c60{font-weight:700;font-size:10pt;font-family:"Calibri"}.c57{font-weight:700;font-size:12pt;font-family:"Aptos"}.c63{text-decoration:none;vertical-align:sub}.c42{text-decoration:none;vertical-align:baseline}.c35{max-width:468pt;padding:72pt 72pt 72pt 72pt}.c1{padding:0;margin:0}.c45{font-style:normal;color:#000000}.c52{vertical-align:baseline}.c51{color:#0a0a0a}.c54{margin-left:54pt}.c31{margin-left:36pt}.c61{text-indent:-18pt}.c44{height:15pt}.c67{vertical-align:super}.c5{height:12pt}.c28{margin-left:72pt}.c48{font-style:normal}.c20{padding-left:0pt}.c17{background-color:#ffffff}.c58{color:#1f1f1f}.c62{color:#000000}.title{padding-top:24pt;color:#000000;font-weight:700;font-size:36pt;padding-bottom:6pt;font-family:"Aptos";line-height:1.1624999999999999;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:18pt;color:#666666;font-size:24pt;padding-bottom:4pt;font-family:"Georgia";line-height:1.1624999999999999;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:12pt;font-family:"Aptos"}p{margin:0;color:#000000;font-size:12pt;font-family:"Aptos"}h1{padding-top:24pt;color:#000000;font-weight:700;font-size:24pt;padding-bottom:6pt;font-family:"Aptos";line-height:1.1624999999999999;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:8pt;color:#0f4761;font-size:16pt;padding-bottom:4pt;font-family:"Play";line-height:1.1624999999999999;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:14pt;color:#000000;font-weight:700;font-size:14pt;padding-bottom:4pt;font-family:"Aptos";line-height:1.1624999999999999;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:12pt;color:#000000;font-weight:700;font-size:12pt;padding-bottom:2pt;font-family:"Aptos";line-height:1.1624999999999999;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:11pt;color:#000000;font-weight:700;font-size:11pt;padding-bottom:2pt;font-family:"Aptos";line-height:1.1624999999999999;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:10pt;color:#000000;font-weight:700;font-size:10pt;padding-bottom:2pt;font-family:"Aptos";line-height:1.1624999999999999;page-break-after:avoid;orphans:2;widows:2;text-align:left}</style></head><body class="c17 c35 doc-content"><div><p class="c47 c5"><span class="c0"></span></p><table class="c8"><tr class="c44"><td class="c24" colspan="1" rowspan="1"><p class="c5 c50"><span class="c3"></span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c5 c49"><span class="c3"></span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c11 c5"><span class="c3"></span></p></td></tr></table><p class="c68 c5"><span class="c3"></span></p></div><p class="c22"><span class="c38">Experiment-2</span></p><p class="c22"><span class="c38">Feedforward Neural Network (MLP)</span></p><p class="c5 c22"><span class="c38"></span></p><p class="c6"><span class="c36">1. Aim</span></p><p class="c12"><span class="c29 c62">To understand the architecture and training process of a Multilayer Perceptron (MLP) for tabular data by implementing it on the Iris dataset, with detailed visualization of forward propagation, backpropagation, and hidden-layer activations for selected samples.</span></p><p class="c12 c5"><span class="c0"></span></p><p class="c6"><span class="c36">2. Theory</span></p><p class="c18"><span class="c29">Deep feedforward networks &mdash; often called feedforward neural networks or multilayer perceptrons (MLPs) &mdash; are fundamental models in deep learning. The goal of a feedforward network is to approximate some target function f *. For example, for a classifier, y = f *(x) maps an input x to a category y.</span></p><p class="c18" id="h.qsapf89kci8c"><span class="c29">A layered feedforward network is one in which any path from an input node to an output node traverses the same number of layers. For example, the n</span><span class="c29 c67">th</span><span class="c29">&nbsp;layer of such a network consists of all nodes that are n edge traversals from an input node. A hidden layer is any layer that is neither the input nor the output layer. A network is fully connected if each node in layer i</span><span class="c19">&nbsp;</span><span class="c29">is connected to all nodes in layer i+1. Layered feedforward networks have become popular because they often generalise well: when trained on a relatively sparse set of examples they frequently provide correct outputs on unseen test data.</span></p><p class="c18" id="h.9kryqamelj0t"><span class="c29">When we use a feedforward neural network to accept an input x</span><span class="c19">&nbsp;</span><span class="c29">and produce an output y&circ;, information flows forward through the network. The inputs </span><span class="c19">x </span><span class="c0">provide the initial information that then propagates to the hidden units at each layer and finally produces y&circ;. This is called forward propagation.</span></p><p class="c18" id="h.uszbnjww6hov"><span class="c0">During training, forward propagation continues until it produces a scalar cost J(&theta;). The backpropagation algorithm (Rumelhart et al., 1986), often simply called backprop, allows information from the cost to flow backwards through the network to compute gradients. The backpropagation algorithm that trains the feedforward neural network can often find a good set of weights (and biases) in a reasonable amount of time. Backpropagation relies on the chain rule to compute derivatives and typically uses a least-squares or cross-entropy criterion depending on the task.</span></p><p class="c9 c5 c17"><span class="c0"></span></p><p class="c9 c5 c17"><span class="c0"></span></p><p class="c9 c17"><span class="c4">Layer-by-Layer (MLP) Transformation:</span></p><ul class="c1 lst-kix_list_18-0 start"><li class="c2 c20 c17 li-bullet-0"><span class="c4">Input Layer: </span><span class="c0">Receives raw input x.</span></li><li class="c2 c20 c17 li-bullet-0" id="h.i0lo9v9q7366"><span class="c4">First Hidden Layer: &nbsp;h</span><span class="c13">1 </span><span class="c4">=g</span><span class="c13">2</span><span class="c4">&nbsp;(W</span><span class="c13">1 </span><img src="images/image1.png"><span class="c4">x </span><img src="images/image2.png"><span class="c4">b</span><span class="c13">1</span><span class="c4">) </span><span class="c0">Applies a linear transformation followed by a nonlinearity.</span></li><li class="c2 c20 c17 li-bullet-0"><span class="c4">Second Hidden Layer (if any): h</span><span class="c13">2</span><span class="c4">&nbsp;=g</span><span class="c13">2</span><span class="c4">(W</span><span class="c13">2</span><img src="images/image1.png"><span class="c4">h</span><span class="c13">1</span><img src="images/image2.png"><span class="c4">b</span><span class="c13">2</span><span class="c4">) </span><span class="c0">Further transforms the representation</span></li><li class="c2 c20 c17 li-bullet-0" id="h.5ttxwywshnfg"><span class="c4">Output Layer: &nbsp;y = g</span><span class="c13">n</span><span class="c4">(W</span><span class="c13">n</span><img src="images/image1.png"><span class="c4">h</span><span class="c13">n-1</span><span class="c4">&nbsp;</span><img src="images/image2.png"><span class="c4">b</span><span class="c13">n</span><span class="c4">) </span><span class="c3">P</span><span class="c0">roduces the final predictions.</span></li></ul><p class="c2"><span class="c0">More generally, the network can be expressed as a composition of functions:</span></p><p class="c2"><img src="images/image3.png"><img src="images/image3.png"><span class="c13">n</span><span class="c3">&#65532;</span><img src="images/image1.png"><span class="c3">&#65532;</span><img src="images/image1.png"><span class="c13">n-1</span><span class="c3">&#65532;</span><img src="images/image1.png"><span class="c4">&nbsp;</span><span class="c3">&#65532;</span><img src="images/image1.png"><span class="c3">&#65532;</span><img src="images/image1.png"><span class="c13">1</span><span class="c4">(x)</span></p><p class="c31 c5 c17 c65"><span class="c3"></span></p><p class="c9"><span class="c3">Where: </span></p><ul class="c1 lst-kix_list_17-0 start"><li class="c9 c54 c20 c17 li-bullet-0"><span class="c4">W</span><span class="c13">i</span><span class="c4">&nbsp;= A</span><span class="c13">i</span><img src="images/image1.png"><span class="c4">x </span><img src="images/image2.png"><span class="c4">b</span><span class="c13">i</span><span class="c25 c63">&nbsp;</span><span class="c0">represents the linear transformation (weights and biases) for layer i.</span></li><li class="c9 c20 c17 c54 li-bullet-0"><span class="c4">&rho;</span><span class="c0">&nbsp;is the activation function (often the same across layers).</span></li></ul><p class="c9 c17" id="h.6n0wt5wr90vi"><span class="c29 c33">Up to now, we&#39;ve been discussing neural networks where the output from one layer is used as input to the next layer. Such networks are called feedforward neural networks. This means there are no loops in the network - information is always feed forward, never feedback which can also be shown as in Fig.1.</span></p><p class="c12"><span class="c0">These models are called feedforward because information flows through the function being evaluated from x, through the intermediate computations used to define f, and finally to the output y. There are no feedback connections in which outputs of the model are fed back into itself.</span></p><p class="c12 c5"><span class="c0"></span></p><ol class="c1 lst-kix_list_19-0 start" start="1"><li class="c2 c20 li-bullet-0"><span class="c4">Gradient Based Learning: </span><span class="c0">For feedforward neural networks, it is important to initialise all weights to small random values; biases may be initialised to zero or to small positive values. Iterative gradient-based optimisation algorithms (e.g., SGD, RMSprop, Adam) are used to train feedforward networks and deepest models.</span></li><li class="c2 c20 li-bullet-0" id="h.i1dteimo6kjd"><span class="c4">Learning XOR: </span><span class="c0">To</span><span class="c4">&nbsp;</span><span class="c0">illustrate the capabilities of feedforward networks, consider the XOR function. XOR returns 1 when exactly one of x1 or x2 &nbsp;is 1, and 0 otherwise. Learning XOR demonstrates that an MLP with a hidden layer can represent non-linearly separable functions.</span></li></ol><p class="c2"><span class="c0">To make the idea of a feedforward network more concrete, we begin with an example of a fully functioning feedforward network on a very simple task: learning the XOR function. The XOR function (&ldquo;exclusive or&rdquo;) is an operation on two binary values, x1 and x2. When exactly one of these binary values is equal to 1, the XOR function returns 1. Otherwise, it returns 0. The XOR function provides the target function y = f*(x) that we want to learn. Our model provides a function y = f(x;&theta;) and our learning algorithm will adapt the parameters &theta; to make f as similar as possible to f*.</span></p><p class="c12 c5"><span class="c3"></span></p><p class="c12"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 597.65px; height: 324.03px;"><img alt="" src="images/image5.png" style="width: 597.65px; height: 324.03px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c26"><span class="c40 c42 c48">Figure 1- Architecture of Feedforward Neural Network</span></p><p class="c22" id="h.910rhii399se"><span class="c40 c42 c48">(Source: M. A. Nielsen, Neural Networks, and Deep Learning. )</span></p><p class="c64"><span class="c29">The process of forward propagation from input to output and backward propagation of errors is repeated several times until the error gets below a predefined threshold. The whole process is represented in the following diagram:</span></p><p class="c26"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 275.00px;"><img alt="" src="images/image6.png" style="width: 624.00px; height: 275.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span class="c40">Figure 2- MLP Process both Forward and Backpropagation </span></p><p class="c22"><span class="c40">(Source: Antonio Gulli, Sujit Pal,</span><span class="c59">&nbsp;</span><span class="c40">Deep Learning with Keras)</span></p><p class="c5 c26"><span class="c15"></span></p><p class="c5 c43"><span class="c3"></span></p><p class="c12"><span class="c29">The forward and backward propagation process is repeated until the error falls below a predefined threshold. The model is updated to progressively minimise the loss function. In a neural network, individual neuron outputs matter less than the collective behaviour of weights in each layer as shown in Fig.2; the network adjusts its internal weights, so the prediction accuracy increases. Using appropriate features and high-quality labels is fundamental for reducing bias and improving learning.</span></p><p class="c6"><span class="c46 c36 c45 c52">Merits of Feedforward Neural Network (MLP):</span></p><ul class="c1 lst-kix_list_16-0 start"><li class="c55 c31 c20 li-bullet-0"><span class="c4">Scalability:</span><span class="c0">&nbsp;The number of hidden layers and neurons can be adjusted to match problem complexity.</span></li><li class="c7 c20 li-bullet-0"><span class="c4">Performance on tabular data:</span><span class="c0">&nbsp;For many structured datasets, MLPs can outperform more complex models due to their simplicity and ability to learn direct features.</span></li><li class="c37 c31 c20 li-bullet-0"><span class="c4">Universal function approximation:</span><span class="c0">&nbsp;MLPs can approximate virtually any continuous function, enabling them to model complex, non-linear relationships.</span></li></ul><p class="c6 c31 c5"><span class="c0"></span></p><p class="c6"><span class="c36 c46">Demerits of Feedforward Neural Network (MLP):</span></p><ul class="c1 lst-kix_list_15-0 start"><li class="c31 c20 c55 li-bullet-0"><span class="c4">Sensitivity to hyperparameters:</span><span class="c0">&nbsp;Performance depends heavily on choices such as number of layers, units, learning rate, and activation functions.</span></li><li class="c2 c20 li-bullet-0"><span class="c4">Overfitting:</span><span class="c0">&nbsp;MLPs can memorise training data and generalise poorly, especially with small datasets.</span></li><li class="c2 c20 li-bullet-0"><span class="c4">Gradient issues:</span><span class="c0">&nbsp;Very deep networks may encounter vanishing or exploding gradients, making training difficult.</span></li><li class="c2 c20 li-bullet-0"><span class="c4">Data requirements:</span><span class="c0">&nbsp;Large amounts of labelled data are often necessary to train effectively.</span></li></ul><p class="c31 c5 c41"><span class="c29 c42 c48 c51"></span></p><p class="c12 c5"><span class="c0"></span></p><p class="c6"><span class="c36">3. Pre-Test (MCQs)</span></p><ol class="c1 lst-kix_list_14-0 start" start="1"><li class="c7 c20 li-bullet-0"><span class="c4">What is the primary goal of a feedforward neural network (MLP)?</span><span class="c3"><br></span><span class="c0">a) To store large datasets</span><span class="c3"><br></span><span class="c0">(Incorrect because storing data is not the network&#39;s objective.)</span><span class="c3"><br></span><span class="c0">b) To approximate a function mapping inputs to output</span><span class="c3"><br></span><span class="c0">(Correct because MLPs are designed to approximate a target function mapping inputs to outputs.)</span><span class="c3"><br></span><span class="c0">c) To create feedback loops</span></li></ol><p class="c7 c61"><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; (Incorrect because feedforward networks do not create feedback loops.)</span></p><p class="c23"><span class="c29">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; d) To sort data efficiently</span></p><p class="c7"><span class="c0">&nbsp;(Incorrect because sorting is an algorithmic task, not the MLP&#39;s goal.)</span></p><p class="c14"><span class="c4">&nbsp; &nbsp; &nbsp;Answer:</span><span class="c0">&nbsp;b</span></p><p class="c23 c5"><span class="c0"></span></p><p class="c23 c5"><span class="c0"></span></p><ol class="c1 lst-kix_list_14-0" start="2"><li class="c7 c20 li-bullet-0"><span class="c4">What do you call a layer that is neither input nor output?</span></li></ol><p class="c7"><span class="c0">a) Input</span></p><p class="c7"><span class="c0">(Incorrect because this is the input layer, not an intermediate one.)</span><span class="c3"><br></span><span class="c0">b) Output</span><span class="c3"><br></span><span class="c0">(Incorrect because this is the final layer producing predictions.)</span><span class="c3"><br></span><span class="c0">c) Hidden</span><span class="c3"><br></span><span class="c0">(Correct because hidden layers are intermediate layers between input and output.)</span><span class="c3"><br></span><span class="c0">d) Bias</span><span class="c3"><br></span></p><p class="c2"><span class="c0">(Incorrect because bias is a parameter, not a layer.)</span><span class="c3"><br></span><span class="c4">Answer:</span><span class="c0">&nbsp;c</span></p><p class="c2 c5"><span class="c0"></span></p><ol class="c1 lst-kix_list_14-0" start="3"><li class="c2 c20 li-bullet-0"><span class="c0">What property lets MLPs model virtually any continuous function?</span><span class="c3"><br></span><span class="c0">a) Linearity</span><span class="c3"><br></span><span class="c0">(Incorrect because linearity limits expressiveness, not enables universality.) </span></li></ol><p class="c2"><span class="c0">b) Depth</span></p><p class="c2"><span class="c0">(Incorrect because depth helps but alone doesn&#39;t state the universal property.)</span></p><p class="c2"><span class="c0">c) Universality</span></p><p class="c2"><span class="c0">&nbsp;(Correct because MLPs have universal function approximation capability.</span></p><p class="c2"><span class="c0">d) Sparsity</span></p><p class="c2"><span class="c0">&nbsp;(Incorrect because sparsity is a regularization trait, not the universal approximation of property.)</span></p><p class="c2"><span class="c0">&nbsp;</span><span class="c4">Answer:</span><span class="c0">&nbsp;c</span></p><p class="c2 c5"><span class="c0"></span></p><ol class="c1 lst-kix_list_14-0" start="4"><li class="c2 c20 li-bullet-0"><span class="c4">Which algorithm computes gradients by propagating errors backward?</span><span class="c0">&nbsp;</span></li></ol><p class="c2"><span class="c0">a) Forward</span></p><p class="c2"><span class="c0">(Incorrect because forward pass computes outputs, not gradients.)</span></p><p class="c2"><span class="c0">b) Gradient</span></p><p class="c2"><span class="c0">&nbsp;(Incorrect because this is a concept, not a specific algorithm.)</span></p><p class="c2"><span class="c0">c)</span><span class="c4">&nbsp;</span><span class="c0">Backpropagation</span></p><p class="c2"><span class="c0">&nbsp;(Correct because backpropagation computes gradients by propagating error backward through the network.)</span></p><p class="c2"><span class="c0">d) Random</span></p><p class="c2"><span class="c0">&nbsp;(Incorrect because random methods do not compute gradients via backprop.)</span></p><p class="c2"><span class="c0">&nbsp;</span><span class="c4">Answer:</span><span class="c0">&nbsp;c</span></p><p class="c2 c5"><span class="c0"></span></p><ol class="c1 lst-kix_list_14-0" start="5"><li class="c2 c20 li-bullet-0"><span class="c4">What type of network has no feedback connections and only forward data flow?</span><span class="c0">&nbsp;</span></li></ol><p class="c2"><span class="c0">a) Recurrent</span></p><p class="c2"><span class="c0">(Incorrect because recurrent networks include feedback loops.)</span></p><p class="c2"><span class="c0">b) Convolutional</span></p><p class="c2"><span class="c0">(Incorrect because convolutional describes layer type, not the presence/absence of feedback.)</span></p><p class="c2"><span class="c0">c)</span><span class="c4">&nbsp;</span><span class="c0">Feedforward</span></p><p class="c2"><span class="c0">(Correct because feedforward networks have no cycles and only forward information flow.)</span></p><p class="c2"><span class="c0">d) Residual</span></p><p class="c2"><span class="c0">(Incorrect because residual refers to skip connections, not the absence of feedback loops.)</span></p><p class="c2"><span class="c4">Answer:</span><span class="c0">&nbsp;c</span></p><p class="c2 c5"><span class="c0"></span></p><ol class="c1 lst-kix_list_14-0" start="6"><li class="c2 c20 li-bullet-0"><span class="c4">What class of values strongly affect MLP performance and must be tuned?</span><span class="c0">&nbsp;</span></li></ol><p class="c2"><span class="c0">a) Weights</span></p><p class="c2"><span class="c0">(Incorrect because weights are learned, not hyperparameters tuned externally.)</span></p><p class="c2"><span class="c0">b) Hyperparameters</span></p><p class="c2"><span class="c0">(Correct because hyperparameters like learning rate and architecture strongly affect performance.)</span></p><p class="c2"><span class="c0">c)</span><span class="c4">&nbsp;</span><span class="c0">Outputs</span></p><p class="c2"><span class="c0">(Incorrect because outputs are results, not tunable configuration parameters.)</span></p><p class="c2"><span class="c0">d) Labels</span></p><p class="c2"><span class="c0">(Incorrect because labels are data, not configuration choices to tune model behavior.)</span></p><p class="c2"><span class="c4">Answer:</span><span class="c0">&nbsp;b</span></p><p class="c2 c5"><span class="c57 c42 c45"></span></p><ol class="c1 lst-kix_list_14-0" start="7"><li class="c2 c20 li-bullet-0"><span class="c4">Which rule is fundamental to computing gradients in backpropagation?</span><span class="c0">&nbsp;</span></li></ol><p class="c2"><span class="c0">a) Addition</span></p><p class="c2"><span class="c0">(Incorrect because addition alone does not compute chained derivatives.)</span></p><p class="c2"><span class="c0">b) Multiplication</span></p><p class="c2"><span class="c0">(Incorrect because multiplication is used but not the core chaining principle.)</span></p><p class="c2"><span class="c0">c)</span><span class="c4">&nbsp;</span><span class="c0">Chain</span></p><p class="c2"><span class="c0">(Correct because the chain rule is used to propagate derivatives through composed functions.)</span></p><p class="c2"><span class="c0">d) Division</span></p><p class="c2"><span class="c0">(Incorrect because division is not the central rule for backpropagating gradients. </span></p><p class="c2"><span class="c4">Answer:</span><span class="c0">&nbsp;c</span></p><p class="c2 c5"><span class="c0"></span></p><ol class="c1 lst-kix_list_14-0" start="8"><li class="c2 c20 li-bullet-0"><span class="c4">What characteristics of deep networks can make training difficult when gradients shrink?</span><span class="c0">&nbsp;</span></li></ol><p class="c2"><span class="c0">a) Exploding</span></p><p class="c2"><span class="c0">(Incorrect because exploding gradients refer to growing gradients, not shrinking.)</span></p><p class="c2"><span class="c0">b) Stable</span></p><p class="c2"><span class="c0">(Incorrect because stability does not describe the problematic shrinking behavior.)</span></p><p class="c2"><span class="c0">c)</span><span class="c4">&nbsp;</span><span class="c0">Vanishing</span></p><p class="c2"><span class="c0">(Correct because vanishing gradients shrink during backprop and hinder learning in deep nets.)</span></p><p class="c2"><span class="c0">d) Sparse</span></p><p class="c2"><span class="c0">(Incorrect because sparsity is unrelated to gradient magnitude issues.)</span></p><p class="c2"><span class="c4">Answer:</span><span class="c0">&nbsp;c</span></p><p class="c2 c5"><span class="c42 c45 c57"></span></p><ol class="c1 lst-kix_list_14-0" start="9"><li class="c2 c20 li-bullet-0"><span class="c4">What term describes a network where each node in layer i connects to all nodes in layer i+1?</span><span class="c0">&nbsp;</span></li></ol><p class="c2"><span class="c0">a) Sparse</span></p><p class="c2"><span class="c0">(Incorrect because sparse implies few connections, not full connectivity.)</span></p><p class="c2"><span class="c0">b) Local</span></p><p class="c2"><span class="c0">(Incorrect because local connectivity restricts connections, unlike full connectivity.)</span></p><p class="c2"><span class="c0">c)</span><span class="c4">&nbsp;</span><span class="c0">Fully connected</span></p><p class="c2"><span class="c0">(Correct because fully connected means every node connects to all nodes in the next layer.)</span></p><p class="c2"><span class="c0">d) Convolutional</span></p><p class="c2"><span class="c0">(Incorrect because convolutional uses local receptive fields, not full connectivity.)</span></p><p class="c2"><span class="c4">Answer:</span><span class="c0">&nbsp;</span></p><p class="c9"><span class="c29">10. &nbsp; </span><span class="c36">&nbsp; &nbsp; &nbsp; What is the role of biases </span><img src="images/image4.png"><span class="c4">&nbsp; &nbsp; each layer?</span></p><p class="c2"><span class="c0">a) Scale</span></p><p class="c2"><span class="c0">(Incorrect because scaling is performed by weights, not biases.)</span></p><p class="c2"><span class="c0">b) Normalize</span></p><p class="c2"><span class="c0">(Incorrect because normalization is a separate operation like BatchNorm.)</span></p><p class="c2"><span class="c0">c)</span><span class="c4">&nbsp;</span><span class="c0">Shift</span></p><p class="c2"><span class="c0">(Correct because biases shift the activation threshold of the linear transform.)</span></p><p class="c2"><span class="c0">d) Dropout</span></p><p class="c2"><span class="c0">(Incorrect because dropout randomly disables units, not a bias function.)</span></p><p class="c7"><span class="c4">Answer:</span><span class="c0">&nbsp;c</span></p><p class="c30 c5 c17"><span class="c21"></span></p><h2 class="c53"><span class="c36 c45">4. Procedure</span></h2><p class="c18"><span class="c36 c45">Objective:</span><span class="c0">&nbsp;Explore the structure and training of MLPs on tabular data by training an MLP on the Iris dataset (4 features, 3 classes) and visualising forward and backprop flows and hidden-layer activations for selected samples.</span></p><p class="c18"><span class="c25">The Iris dataset used contains four input features &mdash; Sepal Length, Sepal Width, Petal Length and Petal Width &mdash; and three classes: </span><span class="c10">Iris-setosa</span><span class="c25">, </span><span class="c10">Iris-versicolor</span><span class="c25">&nbsp;and </span><span class="c10">Iris-virginica</span><span class="c0">.</span></p><p class="c18"><span class="c0">This experiment uses an MLP with one input layer, two hidden layers and one output layer, and visualises forward and backward propagation. Different optimisers (RMSprop, SGD and Adam) are compared to determine which yields the best accuracy.</span></p><p class="c18"><span class="c36 c45">Steps</span></p><ol class="c1 lst-kix_list_2-0 start" start="1"><li class="c20 c27 li-bullet-0"><span class="c4">Import libraries:</span><span class="c0">&nbsp;numpy, pandas, matplotlib for visualisation, and sklearn for data handling, pipelines and evaluation. Use a deep learning framework (e.g., TensorFlow/Keras or PyTorch) for model implementation.</span></li><li class="c2 c20 li-bullet-0"><span class="c4">Dataset loading and description:</span></li></ol><ol class="c1 lst-kix_list_2-1 start" start="1"><li class="c9 c28 c20 li-bullet-0"><span class="c0">Load the Iris dataset (e.g., from Kaggle or sklearn.datasets).</span></li><li class="c9 c20 c28 li-bullet-0"><span class="c0">The dataset shape is (150, 5): four feature columns and one label column.</span></li><li class="c9 c28 c20 li-bullet-0"><span class="c0">Class distribution: Iris-setosa: 50, Iris-versicolor: 50, Iris-virginica: 50 (balanced).</span></li><li class="c9 c28 c20 li-bullet-0"><span class="c0">This dataset has no missing values or duplicate rows; minimal cleaning is required.</span></li><li class="c9 c28 c20 li-bullet-0"><span class="c0">Scale features (e.g., StandardScaler) and encode labels (one-hot encoding).</span></li><li class="c9 c28 c20 li-bullet-0"><span class="c0">Split into train/test sets with 80% training and 20% testing; use stratify to preserve class proportions.</span></li></ol><ol class="c1 lst-kix_list_2-0" start="3"><li class="c2 c20 li-bullet-0"><span class="c4">Initialise parameters and model building:</span></li></ol><ol class="c1 lst-kix_list_2-1 start" start="1"><li class="c9 c28 c20 li-bullet-0"><span class="c0">Typical hyperparameters: epochs=100, batch_size=8, learning_rate=0.01, optimiser=RMSprop (compare with SGD and Adam).</span></li><li class="c9 c28 c20 li-bullet-0"><span class="c0">Build the model with one input layer, two hidden dense layers, and one output layer (softmax). Display the model summary and plot the model architecture.</span></li></ol><ol class="c1 lst-kix_list_2-0" start="4"><li class="c2 c20 li-bullet-0"><span class="c4">Model training:</span></li></ol><ol class="c1 lst-kix_list_2-1 start" start="1"><li class="c9 c28 c20 li-bullet-0"><span class="c0">Train for 100 epochs with mini-batches of size 8. Reserve 20% of the training data for validation.</span></li><li class="c9 c28 c20 li-bullet-0"><span class="c0">Plot training and validation curves for loss and accuracy versus epochs.</span></li><li class="c9 c28 c20 li-bullet-0"><span class="c0">Optionally, visualise forward and backward flows for selected samples and record gradient norms for analysis.</span></li></ol><ol class="c1 lst-kix_list_2-0" start="5"><li class="c2 c20 li-bullet-0"><span class="c4">Model evaluation:</span></li></ol><ol class="c1 lst-kix_list_2-1 start" start="1"><li class="c9 c28 c20 li-bullet-0"><span class="c0">Evaluate the model on the test set using accuracy, precision, recall, and F1 score. Visualise results using a confusion matrix.</span></li><li class="c9 c28 c20 li-bullet-0"><span class="c0">Show a classification report (precision, recall, F1-score, support) and compute macro and weighted averages.</span></li><li class="c9 c28 c20 li-bullet-0"><span class="c0">Analyse misclassifications and class-wise performance.</span></li></ol><ol class="c1 lst-kix_list_2-0" start="6"><li class="c2 c20 li-bullet-0"><span class="c4">Gradient checkpoints (backprop flow):</span></li></ol><ol class="c1 lst-kix_list_2-1 start" start="1"><li class="c28 c20 c34 li-bullet-0"><span class="c0">Record L2 norms of gradients per layer (both hidden layers and output layer) for selected samples and save checkpoints for analysis.</span></li></ol><p class="c30 c5 c17"><span class="c21"></span></p><p class="c30 c5 c17"><span class="c21"></span></p><p class="c5 c17 c30"><span class="c21"></span></p><p class="c6"><span class="c36">5. Post-Test (MCQs)</span></p><p class="c6"><span class="c36">&nbsp; &nbsp;1. How many input features does the Iris dataset have in this experiment?</span><span class="c0">&nbsp;</span></p><p class="c2"><span class="c0">a) Two</span></p><p class="c2"><span class="c0">&nbsp;(Incorrect because Iris has more than two features.)</span></p><p class="c2"><span class="c0">b) Four</span></p><p class="c2"><span class="c0">(Correct because Sepal Length, Sepal Width, Petal Length, and Petal Width are used.)</span></p><p class="c2"><span class="c0">c) Three</span></p><p class="c2"><span class="c0">(Incorrect because there are four numeric features, not three.)</span></p><p class="c2"><span class="c0">d) Five</span></p><p class="c2"><span class="c0">(Incorrect because five would include the label column, but inputs are four.)</span></p><p class="c2"><span class="c4">Answer:</span><span class="c0">&nbsp;b</span></p><p class="c9 c5"><span class="c0"></span></p><p class="c9 c5"><span class="c0"></span></p><p class="c18"><span class="c4">2. Which of the following best describes the Iris dataset class distribution?</span></p><p class="c2"><span class="c0">a) Balanced</span></p><p class="c2"><span class="c0">(Correct because each of the three classes has exactly 50 instances, giving equal class counts. )</span></p><p class="c2"><span class="c0">b) Imbalanced</span></p><p class="c2"><span class="c0">(Incorrect because imbalanced implies unequal class counts, which is not the case here. )</span></p><p class="c2"><span class="c0">c) Skewed</span></p><p class="c2"><span class="c0">(Incorrect because skewed suggests a heavy bias toward one or more classes, which does not apply.)</span></p><p class="c2"><span class="c0">d) Sparse</span></p><p class="c2"><span class="c0">(Incorrect because sparse refers to feature/data sparsity, not equal class counts.)</span></p><p class="c2"><span class="c4">Answer:</span><span class="c0">&nbsp;a</span></p><p class="c2 c5"><span class="c0"></span></p><p class="c9"><span class="c36">3. What does stratify ensure when splitting a classification dataset?</span><span class="c0">&nbsp;</span></p><p class="c2"><span class="c0">a) Shuffle</span></p><p class="c2"><span class="c0">(Incorrect because shuffles randomize order, not preserve class proportions.)</span></p><p class="c2"><span class="c0">b)</span><span class="c4">&nbsp;</span><span class="c0">Balance</span></p><p class="c2"><span class="c0">(Correct because stratify preserves class distribution proportions across splits. )</span></p><p class="c2"><span class="c0">c) Scale</span></p><p class="c2"><span class="c0">(Incorrect because scaling changes feature magnitudes, not class ratios.)</span></p><p class="c2"><span class="c0">d) Encode</span></p><p class="c2"><span class="c0">(Incorrect because encoding changes label representation, not split composition.)</span></p><p class="c2"><span class="c4">Answer:</span><span class="c0">&nbsp;b</span></p><p class="c2 c5"><span class="c0"></span></p><p class="c9 c5"><span class="c0"></span></p><p class="c6 c5"><span class="c4"></span></p><p class="c6"><span class="c36">&nbsp;4. Which technique checks the correctness of backpropagation gradients numerically?</span><span class="c0">&nbsp;</span></p><p class="c2"><span class="c0">a) Dropout</span></p><p class="c2"><span class="c0">(Incorrect because dropout is a regularization method, not a gradient check.)</span></p><p class="c2"><span class="c0">b)</span><span class="c4">&nbsp;</span><span class="c0">Gradient-check</span></p><p class="c2"><span class="c0">(Correct because gradient checking numerically verifies analytic gradients from backprop.</span></p><p class="c2"><span class="c0">c) BatchNorm</span></p><p class="c2"><span class="c0">(Incorrect because batch normalization stabilizes activations, not verifying gradients.)</span></p><p class="c2"><span class="c0">d) EarlyStop</span></p><p class="c2"><span class="c0">(Incorrect because early stopping halts training, not used for gradient verification.)</span></p><p class="c2"><span class="c4">Answer: </span><span class="c0">b</span></p><p class="c2 c5"><span class="c0"></span></p><p class="c2 c5"><span class="c0"></span></p><p class="c9"><span class="c36">5. How many hidden layers does the MLP architecture contain?</span><span class="c0">&nbsp;</span></p><p class="c2"><span class="c0">a) One</span></p><p class="c2"><span class="c0">(Incorrect because the model uses two hidden layers.)</span></p><p class="c2"><span class="c0">b)</span><span class="c4">&nbsp;Two</span></p><p class="c2"><span class="c0">(Correct because the architecture specifies two dense hidden layers.)</span></p><p class="c2"><span class="c0">c) Three</span></p><p class="c2"><span class="c0">(Incorrect because only two hidden layers are used.)</span></p><p class="c2"><span class="c0">d) Zero</span></p><p class="c2"><span class="c0">(Incorrect because the network includes hidden layers.)</span></p><p class="c2"><span class="c4">Answer:</span><span class="c0">&nbsp;b</span></p><p class="c9 c5"><span class="c4"></span></p><p class="c9"><span class="c4">&nbsp; &nbsp; </span></p><p class="c9"><span class="c36">&nbsp;6. Which loss function is appropriate for the compiled MLP on one-hot labels?</span><span class="c0">&nbsp;</span></p><p class="c2"><span class="c0">a) MSE</span></p><p class="c2"><span class="c0">(Incorrect because mean squared error is not ideal for multiclass classification with probabilities.)</span></p><p class="c2"><span class="c0">b) Hinge</span></p><p class="c2"><span class="c0">(Incorrect because hinge loss is for margin classifiers, not softmax outputs.)</span></p><p class="c2"><span class="c0">c) Categorical_crossentropy</span></p><p class="c2"><span class="c0">(Correct because categorical crossentropy matches softmax outputs with one-hot labels.</span></p><p class="c2"><span class="c0">d) MAE</span></p><p class="c2"><span class="c0">(Incorrect because mean absolute error is not standard for multiclass classification.)</span></p><p class="c2"><span class="c4">Answer:</span><span class="c0">&nbsp;c</span></p><p class="c9 c5"><span class="c4"></span></p><p class="c9 c5"><span class="c4"></span></p><p class="c9"><span class="c36">7. What activation is typically used in the output layer for this 3-class problem?</span><span class="c0">&nbsp;</span></p><p class="c2"><span class="c0">a) ReLU</span></p><p class="c2"><span class="c0">(Incorrect because ReLU is for hidden layers, not multiclass outputs.)</span></p><p class="c2"><span class="c0">b) Sigmoid</span></p><p class="c2"><span class="c0">(Incorrect because sigmoid is for binary outputs, not softmax multiclass.)</span></p><p class="c2"><span class="c0">c) Softmax</span></p><p class="c2"><span class="c0">(Correct because softmax produces class probabilities for multiclass classification.</span></p><p class="c2"><span class="c0">d) Tanh</span></p><p class="c2"><span class="c0">(Incorrect because tanh is not suitable for producing class probability distributions.)</span></p><p class="c2"><span class="c0">&nbsp;</span><span class="c4">Answer:</span><span class="c0">&nbsp;c</span></p><p class="c2 c5"><span class="c0"></span></p><p class="c9"><span class="c36">8. Which technique reduces overfitting by adding a penalty proportional to square weights?</span><span class="c0">&nbsp;</span></p><p class="c2"><span class="c0">a) Dropout</span></p><p class="c2"><span class="c0">(Incorrect because dropout randomly disables units, not adding weight penalties.)</span></p><p class="c2"><span class="c0">b)</span><span class="c4">&nbsp;</span><span class="c0">L2</span></p><p class="c2"><span class="c0">(Correct because L2 regularization (weight decay) penalizes the squared magnitude of weights.</span></p><p class="c2"><span class="c0">c) L1</span></p><p class="c2"><span class="c0">(Incorrect because L1 penalizes absolute weights and encourages sparsity, not squared penalty.)</span></p><p class="c2"><span class="c0">d) BatchNorm</span></p><p class="c2"><span class="c0">(Incorrect because batch normalization normalizes activations, not directly penalize weights.)</span></p><p class="c2"><span class="c4">Answer:</span><span class="c0">&nbsp;b</span></p><p class="c9 c5"><span class="c4"></span></p><p class="c9"><span class="c36">9. &nbsp; Which optimizers are compared to finding the best accuracy?</span><span class="c0">&nbsp;</span></p><p class="c2"><span class="c0">a) Evolutionary</span></p><p class="c2"><span class="c0">(Incorrect because evolutionary optimizers are not part of the comparison.)</span></p><p class="c2"><span class="c0">b)</span><span class="c4">&nbsp;</span><span class="c0">RMSprop/SGD/Adam</span></p><p class="c2"><span class="c0">(Correct because RMSprop, SGD, and Adam are the gradient-based optimizers compared.</span></p><p class="c2"><span class="c0">c) Bayesian</span></p><p class="c2"><span class="c0">(Incorrect because Bayesian optimizers are not mentioned for this comparison.)</span></p><p class="c2"><span class="c0">d) Random</span></p><p class="c2"><span class="c0">(Incorrect because random search is not the optimization of family compared.)</span></p><p class="c2"><span class="c4">Answer:</span><span class="c0">&nbsp;b</span></p><p class="c2 c5"><span class="c0"></span></p><p class="c2 c5"><span class="c0"></span></p><p class="c9"><span class="c36">10. What</span><span class="c42 c45 c60">&nbsp;does forward propagation produce before loss of computation?</span></p><p class="c2"><span class="c0">a) Gradients</span></p><p class="c2"><span class="c0">(Incorrect because gradients are computed during backprop, after loss.) </span></p><p class="c2"><span class="c0">b)</span><span class="c4">&nbsp;</span><span class="c0">Predictions</span></p><p class="c2"><span class="c0">(Correct because forward propagation computes predictions from inputs.</span></p><p class="c2"><span class="c0">c) Regularization</span></p><p class="c2"><span class="c0">(Incorrect because regularization modifies training but does not produce predictions.)</span></p><p class="c2"><span class="c0">d) Normalization</span></p><p class="c2"><span class="c0">(Incorrect because normalization preprocesses inputs but does not produce.)</span></p><p class="c2"><span class="c4">Answer:</span><span class="c0">&nbsp;b</span></p><p class="c9 c5"><span class="c4"></span></p><p class="c9 c5"><span class="c4"></span></p><p class="c56"><span class="c36">6. </span><span class="c42 c45 c66">References</span></p><ul class="c1 lst-kix_list_1-0 start"><li class="c31 c20 c39 li-bullet-0"><span class="c0">P. J. Werbos, &ldquo;Backpropagation through time: what it does and how to do it,&rdquo; </span><span class="c10 c42">Proceedings of the IEEE</span><span class="c0">, vol. 78, no. 10, pp. 1550&ndash;1560, 1990.</span></li><li class="c7 c20 li-bullet-0"><span class="c0">I. Goodfellow, Y. Bengio, and A. Courville, </span><span class="c10 c42">Deep Learning</span><span class="c0">. Cambridge, MA, USA: MIT Press, 2016.</span></li><li class="c31 c20 c37 li-bullet-0"><span class="c0">M. A. Nielsen, </span><span class="c10 c42">Neural Networks, and Deep Learning</span><span class="c0">. Determination Press (online), 2015.</span></li></ul><p class="c5 c6"><span class="c4"></span></p><p class="c18 c5 c31"><span class="c3"></span></p><p class="c7 c5"><span class="c0"></span></p><p class="c18 c5"><span class="c0"></span></p><p class="c18 c5"><span class="c4"></span></p><p class="c18 c5"><span class="c0"></span></p><p class="c18 c5"><span class="c0"></span></p><p class="c18"><span class="c0">&nbsp; &nbsp;</span></p><p class="c18 c5"><span class="c0"></span></p><p class="c23 c5"><span class="c0"></span></p><p class="c23 c5"><span class="c0"></span></p><p class="c23 c5"><span class="c4"></span></p><p class="c23"><span class="c0">&nbsp; </span></p><p class="c23 c5"><span class="c0"></span></p><p class="c23 c5"><span class="c32"></span></p><p class="c16 c5"><span class="c0"></span></p><p class="c5 c16"><span class="c0"></span></p><p class="c6 c5"><span class="c0"></span></p><p class="c6 c5"><span class="c4"></span></p><p class="c6 c5"><span class="c36 c42 c48 c58"></span></p><p class="c6"><span class="c21">&nbsp;</span></p><p class="c2 c5"><span class="c0"></span></p><div><p class="c5 c47"><span class="c3"></span></p><table class="c8"><tr class="c44"><td class="c24" colspan="1" rowspan="1"><p class="c50 c5"><span class="c3"></span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c49 c5"><span class="c3"></span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c5 c11"><span class="c3"></span></p></td></tr></table><p class="c5 c68"><span class="c3"></span></p></div></body></html>